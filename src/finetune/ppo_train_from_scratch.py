from transformers import (
    AutoModelForCausalLM,
    AutoModel,
    AutoModelForSequenceClassification,
    AutoTokenizer,
)
from dataclasses import dataclass
from typing import Optional, Union, Tuple
import random
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter


from typing import List, Tuple, Union, Dict, Optional

from src.configs.config import MODEL_PATH, REWARD_MODEL_PATH


# Builddataset
class PromptDataset(Dataset):
    def __init__(self, prompts, tokenizer: AutoTokenizer, apply_chat_template=False):
        self.prompts = prompts
        self.tokenizer: AutoTokenizer = tokenizer

        self.final_prompts = []

        for prompt in prompts:
            if apply_chat_template:
                content = [{"role": "user", "content": prompt}]
                prompt = self.tokenizer.apply_chat_template(
                    content, tokenize=False, add_generation_prompt=True
                )
            else:
                prompt = self.tokenizer.bos_token + prompt

            self.final_prompts.append(prompt)

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        return self.final_prompts[idx]


# Value (Critic) Model: predicts the value of each step's action during token generation, initialized using the actor model with an additional regression head. Output shape: (batch_size, seq_len, 1)
class Critic(nn.Module):
    """
    Value (Critic) Model:
    - Predicts the reward generated by each action (token generation step)
    - Initialized using the actor model with an additional regression head
    - Output shape: (batch_size, seq_len, 1)
    - Obtained through policy model initialization
    """

    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model  # Can be viewed as an encoder
        self.base_model.eval()  # Freeze the backbone part
        self.value_head = nn.Linear(base_model.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask, num_actions):
        """
        num_actions: Number of tokens newly inferred by the model

        return values: shape = (batch_size, num_actions)
        """

        hidden_states = self.base_model.forward(
            input_ids, attention_mask=attention_mask
        ).last_hidden_state  # shape = (batch_size, seq_len, hidden_size)

        value_model_output = self.value_head.forward(
            hidden_states
        )  # shape = (batch_size, seq_len, 1)

        values = value_model_output.squeeze(-1)[
            :, -num_actions:
        ]  # Only take the actions (response) part made by the policy

        return values


def compute_policy_loss(
    log_probs: torch.Tensor,
    old_log_probs: torch.Tensor,
    advantages,
    action_mask=None,
    clip_eps=0.2,
):
    """
    advantage:
        Advantage function used to measure the advantage of the current policy relative to the old policy
        advantage.shape = (batch_size, seq_len)

    log_probs:
        Logits output from the policy model
        log_probs.shape = (batch_size, seq_len)

    old_log_probs:
        Logits output from the old policy model
        old_log_probs.shape = (batch_size, seq_len)

    action_mask:
        Indicates which actions are valid/effective and which are invalid/ineffective
        action_mask.shape = (batch_size, seq_len)
    """
    ratio = (log_probs - old_log_probs).exp()  # A/B = exp[log(A/B)] = exp[logA - LogB]

    # ratio.shape = (batch_size, seq_len)

    surrogate1 = ratio * advantages
    surrogate2 = ratio.clamp(1.0 - clip_eps, 1.0 + clip_eps) * advantages

    loss = -torch.min(surrogate1, surrogate2)  # shape =  (bsz, seq_len)

    if action_mask is None:
        return loss.mean(
            -1
        ).mean()  # First average over sequence dimension, then over batch dimension
    else:
        return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()


def compute_value_loss(
    values, old_values, returns, action_mask=None, clip_eps: float = None
):
    """
    Compute the loss function for the value model (Value Loss), used to optimize the critic model.

    Parameters:
        values (torch.Tensor): Current state values predicted by the value model, shape=(batch_size, seq_len). State values (V(s)) predicted by the critic model, representing expected returns starting from the current state under the current policy
        old_values (torch.Tensor): State values predicted by the old value model, shape=(batch_size, seq_len)
        returns (torch.Tensor): Actually observed returns, shape=(batch_size, seq_len). Actually observed discounted returns (G_t)
        action_mask (torch.Tensor, optional): Action mask indicating which positions are valid/effective actions, shape=(batch_size, seq_len)
        clip_eps (float, optional): Clipping coefficient used to restrict value updates. If None, no clipping is used

    Returns:
        torch.Tensor: Computed scalar value loss

    Loss Function:
        loss = (values - returns) ** 2
        Equivalent to L = (V(s) - G_t)²

        Essence:
            Mean squared error (MSE) loss computation for value function (critic) training

        Purpose:
            1. This is typical value function fitting in temporal difference learning
            2. The goal is to minimize the squared error between predicted values and actual returns
            3. By minimizing this loss, the critic model gradually learns to more accurately predict the expected returns for each state

        Components:
            values: State values (V(s)) predicted by the critic model, representing expected returns starting from the current state under the current policy
            returns: Actually observed discounted returns (G_t), calculated using Monte Carlo methods or TD(λ) methods

    Implementation details:
        1. When clip_eps is not None, use PPO's clipping mechanism to prevent excessive value function updates:
           - Compute clipped value predictions: values_clipped = old_values + clip(values - old_values)
           - Take the maximum between clipped and unclipped losses: loss = max((values_clipped-returns)^2, (values-returns)^2)
        2. When clip_eps is None, directly compute MSE loss: loss = (values - returns)^2
        3. If action_mask exists, only compute loss for valid/effective positions
        4. Finally average over sequence and batch dimensions to get scalar loss value

    Mathematical formulas:
        clipped_loss = (values_clipped - returns)^2
        unclipped_loss = (values - returns)^2
        loss = max(clipped_loss, unclipped_loss)  # When using clipping
        or
        loss = unclipped_loss  # When not using clipping
    """
    if clip_eps is not None:
        values_clipped = old_values + (values - old_values).clamp(-clip_eps, clip_eps)
        surrogate1 = (values_clipped - returns) ** 2
        surrogate2 = (values - returns) ** 2

        loss = torch.max(surrogate1, surrogate2)

    else:
        loss = (values - returns) ** 2

    if action_mask is None:
        return loss.mean(
            -1
        ).mean()  # First average over sequence dimension, then over batch dimension

    else:
        return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()


def compute_entropy(
    log_probs: torch.Tensor, action_mask: Optional[torch.Tensor] = None
):
    """
    Compute the entropy of the policy to encourage exploration

    Parameters:
        log_probs: Log probabilities output from the policy model, shape=(batch_size, num_actions)
        action_mask: Action mask indicating which positions are valid/effective actions, shape=(batch_size, num_actions)

    Returns:
        torch.Tensor: Computed scalar entropy value

    Formula:
        entropy = -sum(exp(log_probs) * log_probs)

    Notes:
        1. Entropy computation is performed along the action dimension (num_actions)
        2. The final return is the average entropy value across the entire batch
    """
    probs = log_probs.exp()
    per_token_entropy = -(probs * log_probs)  # shape = (bsz, num_actions)
    entropy = per_token_entropy.sum(-1)  # shape = (bsz,)

    if action_mask is None:
        return entropy.mean()
    else:
        return ((per_token_entropy * action_mask).sum(-1)).mean()


class ExperienceBuffer:
    """
    Experience buffer

    Purpose:
        Store the trajectories we sampled
    """

    def __init__(self, limit):
        self.limit = limit
        self.buffer = (
            []
        )  # List to store trajectories, which is the actual trajectory dataset D

    def append(self, experiences: List["Experience"]):
        """
        Remove old data, keep new data

        Purpose:
            Convert the experiences object list to batch dictionary list

            Then add the batch to the buffer
        """
        batch = [{} for _ in range(len(experiences))]

        keys = (
            "seqs",
            "action_log_probs",
            "values",  # State values (V(s)) predicted by the critic model, shape = (batch_size, seq_len)
            "returns",  # Actually observed discounted returns (G_t), shape = (batch_size, seq_len)
            "advantages",
            "attention_mask",
            "action_mask",
            "num_actions",  # Number of tokens newly inferred by the model
        )

        for key in keys:
            for i, experience in enumerate(experiences):
                value = getattr(experience, key)
                batch[i][key] = value

        self.buffer.extend(batch)

        if len(self.buffer) >= self.limit:
            self.buffer = self.buffer[len(self.buffer) - self.limit :]

    def get_batches(self, batch_size):
        """
        Randomly sample a batch
        """
        batch = random.sample(self.buffer, batch_size)
        return batch

    def clear(self):
        self.buffer = []

    def __len__(self):
        return len(self.buffer)

    def __getitem__(self, index):
        return self.buffer[index]


@dataclass
class Samples:
    """
    Store the output of the policy model
    """

    seqs: torch.Tensor
    attention_mask: Optional[torch.LongTensor]
    action_mask: Optional[torch.BoolTensor]  # shape = (batch_size, seq_len)
    num_actions: Union[int, torch.Tensor]
    packed_seq_lens: Optional[torch.Tensor]
    response_length: torch.Tensor
    total_length: torch.Tensor


@dataclass
class Experience:
    seqs: torch.Tensor
    action_log_probs: torch.Tensor
    values: torch.Tensor
    returns: Optional[torch.Tensor]
    advantages: Optional[torch.Tensor]
    attention_mask: Optional[torch.LongTensor]
    action_mask: Optional[torch.BoolTensor]
    reward: torch.Tensor
    response_length: torch.Tensor
    total_length: torch.Tensor
    num_actions: Union[int, torch.Tensor]
    kl: Optional[torch.Tensor] = None


def compute_approx_kl(
    log_probs: torch.Tensor,  # Logits output from the policy model
    ref_log_probs: torch.Tensor,  # Logits output from the reference model
    action_mask: Optional[torch.Tensor] = None,
):
    """
    log_probs.shape = (batch_size, seq_len)

    return log_ratio, shape = (batch_size, seq_len)
    """
    log_ratio = log_probs.float() - ref_log_probs.float()  # log(A/B) = logA - logB

    if action_mask is not None:
        log_ratio = log_ratio * action_mask

    return log_ratio


def get_advantages_and_returns(
    values: torch.Tensor,
    rewards: torch.Tensor,
    action_mask: torch.Tensor,
    gamma: float,
    lambd: float,
):
    """
    ### Args:
        values: State values (V(s)) predicted by the value model, shape = (batch_size, seq_len)
        rewards: Rewards (R_t) predicted by the reward model, shape = (batch_size, seq_len)
        action_mask: Action mask indicating which positions are valid/effective actions, shape=(batch_size, seq_len)
        gamma: Discount factor



    ### Principle:
        # A(t) = R(t) + gam*V(t+1) - V(t)
        # gae:A(t) = R(t) + gam*V(t+1) - V(t) + gam*lam*A(t+1)
        # Future advantage and future value at the last time step are 0: A(T+1) = 0, V(T+1) = 0, then A(T) = R(T) - V(T), deriving A(T)
        # A(T-1) = R(T-1) + gam*V(T) - V(T-1) + gam*lam*A(T). Knowing A(T), we can compute A(T-1) and so on
        # returns(t) = A(t) + V(t) = = R(t) + gam * (V(t+1) + lam * A(t+1))


        returns(t) = A(t) + V(t)
            = [R(t) + gamma*V(t+1) - V(t)] + V(t)  # Substitute regular advantage function definition
            = R(t) + gamma*V(t+1)  # Standard return formula

        returns(t) = A(t) + V(t)
            = [R(t) + gamma*V(t+1) - V(t) + gam*lam*A(t+1)] + V(t)  # Substitute generalized advantage function definition
            = R(t) + gamma*V(t+1) + gam*lam*A(t+1)  # Return formula

    ### Return: (advantages, returns)
        advantages: Advantage function used to measure the advantage of the current policy relative to the old policy
            shape = (batch_size, seq_len)

        returns: Actually observed discounted returns (G_t)
            shape = (batch_size, seq_len)

    """

    last_gae_lam = 0  # Initialize A_{T+1} to 0, because future advantage and future value at the last time step are 0: A(T+1) = 0, V(T+1) = 0

    advantages_reversed = (
        []
    )  # Since we start computing from the last step's advantage A_T = delta + gamma*lambda* A_{T+1}, we need to reverse the result at the end

    response_length = rewards.size(
        1
    )  # Length of all actions in a trajectory (or length of all response tokens)

    if action_mask is not None:
        values = action_mask * values
        rewards = action_mask * rewards

    for t in reversed(range(response_length)):

        nextvalues = (
            values[:, t + 1] if t < response_length - 1 else 0.0
        )  # V(t+1)    shape = (batch_size)
        delta = (
            rewards[:, t] + gamma * nextvalues - values[:, t]
        )  # delta = R(t) + gam*V(t+1) - V(t)     shape = (batch_size)

        last_gae_lam = (
            delta + gamma * lambd * last_gae_lam
        )  # A(t) = delta + gam*lam*A(t+1) = R(t) + gam*V(t+1) - V(t) + gam*lam*A(t+1)       # shape  = (batch_size)

        advantages_reversed.append(
            last_gae_lam
        )  # List[torch(batch,) for _ in range(response_length)]

    advantages = torch.stack(
        advantages_reversed[::-1], dim=1
    )  # shape = (batch_size, response_length)

    returns = advantages + values  # Basic expected return formula

    return advantages.detach(), returns


def generate_samples(
    prompts,
    model,
    max_length,
    max_new_tokens,
    n_samples_per_prompt,
    micro_rollout_batch_size,  # Batch size for the trajectory dataset D
) -> List[Samples]:
    """
    Sample trajectory data for an entire dataset D

    return List[Samples]

        where len(Samples) == micro_rollout_batch_size
    """

    samples_list = []
    model.eval()
    all_prompts = sum(
        [[prompt] * n_samples_per_prompt for prompt in prompts], []
    )  # Return value: a list containing each prompt repeated n_samples_per_prompt times

    for i in range(0, len(all_prompts), micro_rollout_batch_size):

        prompts = all_prompts[i : i + micro_rollout_batch_size]
        inputs = actor_tokenizer(
            prompts,
            padding="max_length",
            max_length=max_length,
            truncation=True,
            return_tensors="pt",
        )
        input_ids = inputs["input_ids"]
        seqs = model.generate(
            **inputs.to(device),
            max_new_tokens=max_new_tokens,
            eos_token_id=eos_token_id,
            pad_token_id=pad_token_id,
        )  # Generate complete/intact trajectories

        # seqs.shape = (micro_rollout_batch_size, max_new_tokens + max_length)

        if seqs.size(1) >= max_new_tokens + max_length:
            seqs = seqs[:, : max_new_tokens + max_length]  # Truncate
        else:
            seqs = torch.cat(
                [
                    seqs,
                    torch.full(
                        (seqs.size(0), max_new_tokens + max_length - seqs.size(1)),
                        fill_value=pad_token_id,
                        device=seqs.device,
                    ),
                ],
                dim=1,
            )  # Pad to full length

        attention_mask = (seqs.ne(pad_token_id)).to(dtype=torch.long)
        ans = seqs[:, input_ids.size(1) :]

        action_mask = (ans.ne(pad_token_id) & ans.ne(eos_token_id)).to(
            dtype=torch.long
        )  # The position of the last valid/effective action is the same in each sequence

        samples = Samples(
            seqs=seqs,
            attention_mask=attention_mask,
            action_mask=action_mask,
            num_actions=action_mask.size(1),
            packed_seq_lens=None,
            response_length=action_mask.float().sum(dim=-1),
            total_length=attention_mask.float().sum(dim=-1),
        )

        samples_list.append(samples)

        return samples_list


def compute_rewards(kl, r, action_mask, kl_ctl, clip_reward_value):
    """
    ### Args:
        kl: Logits output from the policy model, shape = (bsz, num_actions)
        r: Reward (R_t) output from the reward model, shape = (batch_size, )
        action_mask: Action mask indicating which positions are valid/effective actions, shape=(batch_size, num_actions)
        kl_ctl: Control coefficient (weight) for KL divergence
        clip_reward_value: Reward clipping threshold

    ### Structure of the final returned reward tensor:
        Most tokens' reward = KL penalty
        Last valid/effective token's reward = KL penalty + clipped reward model output

    ### Return:
        rewards: Reward，shape = (bsz, num_actions)

    ### Note/Attention:
        Here, we simplified the reward calculation for each token, only considering them as -KL divergence (if it's the last token in the sequence, we also add the reward model's prediction score),
            but in actual applications, we would use more detailed reward models to predict the reward for each token.
    """

    kl_divergence_estimate = (
        -kl_ctl * kl
    )  # Negative sign indicates penalty for large KL divergence
    rewards = kl_divergence_estimate  # shape = (bsz, num_actions)    # Initial reward = KL penalty

    ends = action_mask.sum(1) + 1  # shape=(bsz,)

    if not isinstance(clip_reward_value, torch.Tensor):
        clip_reward_value = torch.tensor(clip_reward_value).to(r.device)
    # Clip the reward for the last token of each sequence
    reward_clip = torch.clamp(
        r, -clip_reward_value, clip_reward_value
    )  # shape = (bsz,)

    # Add the clipped final reward to the last valid/effective token of each sequence
    batch_size = r.size(0)
    for j in range(batch_size):
        rewards[j, : ends[j]][-1] += reward_clip[
            j, 0
        ]  # Only add reward at the end of sequence

    return rewards


def generate_experiences(samples_list: List[Samples]) -> List[Experience]:

    actor_model.eval()
    ref_model.eval()
    reward_model.eval()
    critic_model.eval()

    experiences = []

    for samples in samples_list:
        seqs = (
            samples.seqs
        )  # seqs.shape = (micro_rollout_batch_size, max_new_tokens + max_length)
        attention_mask = samples.attention_mask
        action_mask = samples.action_mask
        num_actions = samples.num_actions

        with torch.no_grad():
            # Compute token probabilities from policy model output
            output = actor_model(seqs, attention_mask=attention_mask)

            logits = (
                output.logits
            )  # shape = (bsz, max_len + max_new_tokens, vocab_size)
            # Why take up to -1: This is the standard "teacher forcing" approach, we don't need the last token's prediction result when predicting the next token
            log_probs = F.log_softmax(
                logits[:, :-1, :], dim=-1
            )  # shape = (bsz, max_len + max_new_tokens - 1, vocab_size)
            # Why start from 1: We need to get the probability of each token predicting the next token (i.e., use the first n-1 tokens to predict the nth token)
            # Gather operation:
            #     Index on the last dimension (dim=-1, i.e., vocab dimension)
            #     Effect: For each position in each batch, select the log probability corresponding to the actually generated token
            #     Equivalent to: log_probs_labels[i,j] = log_probs[i,j,seqs[i,j]], but seqs[i,j] actually contains the token id at position seqs[i,j+1]
            log_probs_labels = log_probs.gather(
                dim=-1, index=seqs[:, 1:].unsqueeze(-1)
            )  # shape = (bsz, max_len + max_new_tokens - 1, 1)
            action_log_probs = log_probs_labels.squeeze(-1)[
                :, -num_actions:
            ]  # shape = (bsz, num_actions)

            # Compute token probabilities from reference model output
            ref_output = ref_model(seqs, attention_mask=attention_mask)
            ref_logits = ref_output.logits
            ref_log_probs = F.log_softmax(ref_logits[:, :-1, :], dim=-1)

            ref_log_probs_labels = ref_log_probs.gather(
                dim=-1, index=seqs[:, 1:].unsqueeze(-1)
            )  # shape = (bsz, max_len + max_new_tokens - 1, 1)

            ref_action_log_probs = ref_log_probs_labels.squeeze(-1)[:, -num_actions:]

            # Compute values

            value = critic_model.forward(seqs, attention_mask, num_actions).to(
                device
            )  # shape = (bsz, num_actions)

            # Transform to text
            seq_texts = actor_tokenizer.batch_decode(seqs, skip_special_tokens=True)

            reward_model_inputs = reward_tokenizer(
                seq_texts, return_tensors="pt", padding=True
            )

            r = reward_model(
                **reward_model_inputs.to(device)
            ).logits  # shape = (bsz, )  # Reward model output, equivalent to the reward score for generating the last token (result reward model)

            # Compute KL divergence
            kl = compute_approx_kl(
                action_log_probs, ref_action_log_probs, action_mask=action_mask
            ).to(
                device
            )  # shape = (bsz, num_actions)

            # Compute actual rewards

            rewards = compute_rewards(
                kl, r, action_mask, kl_ctl=0.1, clip_reward_value=0.2
            )  # shape = (bsz, num_actions)

            # Compute advantages and returns
            advantages, returns = get_advantages_and_returns(
                value, rewards, action_mask, gamma=0.1, lambd=0.2
            )

        # actor_model.train()
        # critic_model.train()
        experiences.append(
            Experience(
                seqs,
                action_log_probs.detach(),
                value.detach(),
                returns.detach(),
                advantages.detach(),
                attention_mask,
                action_mask,
                r.detach(),
                samples.response_length,
                samples.total_length,
                num_actions,
                kl.detach(),
            )
        )

    return experiences


@dataclass
class BufferItem:
    seqs: torch.Tensor
    action_log_probs: torch.Tensor
    values: torch.Tensor
    returns: torch.Tensor
    advantages: torch.Tensor
    attention_mask: torch.Tensor
    action_mask: torch.Tensor
    num_actions: Union[int, torch.Tensor]


def collate_fn(batch):
    """
    batch: List[Experience]  -> List[Samples]

    len(batch) == micro_train_batch_size == 2

    len(Samples) == micro_rollout_batch_size == 2

    A batch contains micro_train_batch_size * micro_rollout_batch_size trajectory samples
    """
    seqs: List[List] = []  # [ [seq1, seq2], [seq3, seq4] ]
    action_log_probs = []
    values = []
    returns = []
    advantages = []
    attention_mask = []
    action_mask = []

    for x in batch:
        seqs.append(x["seqs"])
        action_log_probs.append(x["action_log_probs"])
        values.append(x["values"])
        returns.append(x["returns"])
        advantages.append(x["advantages"])
        attention_mask.append(x["attention_mask"])
        action_mask.append(x["action_mask"])

    seqs = torch.cat(
        seqs, dim=0
    )  # Vertically concatenate two sequences in the row direction to form a matrix
    action_log_probs = torch.cat(action_log_probs, dim=0)
    values = torch.cat(values, dim=0)
    returns = torch.cat(returns, dim=0)
    advantages = torch.cat(advantages, dim=0)
    attention_mask = torch.cat(attention_mask, dim=0)
    action_mask = torch.cat(action_mask, dim=0)

    return BufferItem(
        seqs,
        action_log_probs,
        values,
        returns,
        advantages,
        attention_mask,
        action_mask,
        action_mask.size(1),
    )


def train_step(experience: Experience, steps):
    """
    Perform training for one mini-batch
    """

    actor_model.train()
    optimizer_actor.zero_grad()

    sequences = experience.seqs
    old_action_log_probs = (
        experience.action_log_probs
    )  # Meaning: original logits from actor model output
    advantages = experience.advantages  # shape = (batch_size, num_actions)
    num_actions = experience.num_actions
    attention_mask = experience.attention_mask
    action_mask = experience.action_mask
    old_values = experience.values
    returns = experience.returns

    logits = actor_model(sequences, attention_mask=attention_mask).logits

    log_probs = F.log_softmax(
        logits[:, :-1, :], dim=-1
    )  # shape = (batch_size, max_len + max_new_tokens - 1, vocab_size)
    log_probs_labels = log_probs.gather(
        dim=-1, index=sequences[:, 1:].unsqueeze(-1)
    )  # shape = (batch_size, max_len + max_new_tokens - 1, 1)
    action_log_probs = log_probs_labels.squeeze(-1)[
        :, -num_actions:
    ]  # shape = (batch_size, num_actions)

    # Compute policy loss
    policy_loss = compute_policy_loss(
        action_log_probs, old_action_log_probs, advantages, action_mask=action_mask
    )

    # Compute entropy
    entropy = compute_entropy(
        action_log_probs, action_mask
    )  # Compute average entropy for a mini-batch

    # Compute value loss
    critic_model.train()
    optimizer_critic.zero_grad()
    values = critic_model.forward(
        sequences, attention_mask, num_actions
    )  # shape = (batch_size, num_actions)
    value_loss = compute_value_loss(values, old_values, returns, action_mask)

    # Merge 3 types of losses
    total_loss = (
        policy_loss + 0.5 * value_loss + 0.01 * entropy
    )  # Coefficients can be adjusted as needed

    # BackwardPropagation
    total_loss.backward()
    optimizer_actor.step()
    optimizer_critic.step()

    # policy_loss.backward()
    # optimizer_actor.step()

    # writer.add_scalar("policy_loss", policy_loss.item(), steps)
    # value_loss.backward()
    # optimizer_critic.step()
    # writer.add_scalar("value_loss", value_loss.item(), steps)

    # Log metrics
    writer.add_scalar("policy_loss", policy_loss.item(), steps)
    writer.add_scalar("value_loss", value_loss.item(), steps)
    writer.add_scalar("entropy", entropy.item(), steps)
    writer.add_scalar("total_loss", total_loss.item(), steps)

    # print(f"step:{steps} policy_loss:{policy_loss.item():.4f} value_loss:{value_loss.item():.4f}" )
    print(
        f"step:{steps} policy_loss:{policy_loss.item():.4f} value_loss:{value_loss.item():.4f} entropy:{entropy.item():.4f} total_loss:{total_loss.item():.4f}"
    )


def train():
    # Initialize experience buffer, the experience buffer is the trajectory dataset D (Trajectories) mentioned in the paper
    buffer = ExperienceBuffer(
        limit=100
    )  # First take 100 prompts to generate trajectories

    steps = 0

    for episode in range(episodes):
        for rand_prompts in prompts_dataloader:
            # Generate samples (get model inference results)
            # Sample trajectories for dataset D
            samples: List[Samples] = generate_samples(
                rand_prompts,
                actor_model,
                max_length,
                max_new_tokens,
                n_samples_per_prompt,
                micro_rollout_batch_size,
            )

            # Generate experiences (get advantages, rewards, returns, etc.)
            experiences = generate_experiences(
                samples
            )  # Compute rewards, returns, advantages, etc. for each trajectory in trajectory dataset D

            buffer.append(
                experiences
            )  # Buffer only stores trajectory data for one dataset D

            dataloader = DataLoader(
                buffer,
                batch_size=micro_train_batch_size,
                shuffle=True,
                collate_fn=collate_fn,
            )

            torch.cuda.empty_cache()

            for epoch in range(
                max_epochs
            ):  # Each dataset D needs to be trained for max_epochs rounds
                for experience in dataloader:  # Each experience is a mini-batch
                    train_step(experience, steps)
                    steps += 1

            buffer.clear()
            torch.cuda.empty_cache()


if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # Total number of iterations
    episodes = 3
    # Number of training rounds for generating one experience (i.e., trajectories dataset)
    max_epochs = 5
    # How much data to take from the prompt dataset at once for generating experience
    rollout_batch_size = 8
    # How much data to take at once for generating experience (generating experience requires multiple model inferences, high memory requirements) # mini-batch
    micro_rollout_batch_size = 2
    # How many samples to generate for one prompt
    n_samples_per_prompt = 2

    # The prompt_dataset below can be seen as a larger dataset D', from which we take a subset D each time

    # Size of trajectory dataset D = rollout_batch_size * n_samples_per_prompt

    # Each time take a mini-batch (micro_rollout_batch_size, or micro_train_batch_size) trajectories from D to update actor_model (online policy)

    # Maximum generation length, equivalent to maximum number of actions, the larger the value, the more exploration possibilities for the model
    max_new_tokens = 50
    # Maximum length
    max_length = 256
    # Actual training batch size, how much data to take at once for parameter updates
    micro_train_batch_size = 2  # mini-batch
    # Log recording
    writer = SummaryWriter("./runs")
    # Policy model
    actor_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH).to(device)
    # Reference model
    ref_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH).to(device)
    # Reward model, how to use: score = reward_model(**input_ids).logits  # shape = (bsz, )
    reward_model = AutoModelForSequenceClassification.from_pretrained(
        REWARD_MODEL_PATH
    ).to(device)

    # actor_tokenizer = AutoTokenizer.from_pretrained('/home/user/Downloads/Qwen3-0.6B-Instruct')
    actor_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_PATH)
    # Value model
    critic_model = Critic(actor_model.base_model).to(device)

    # Initialize optimizers
    optimizer_actor = torch.optim.Adam(actor_model.parameters(), lr=0.00005)
    optimizer_critic = torch.optim.Adam(critic_model.parameters(), lr=0.00005)

    # Padding method is left padding
    actor_tokenizer.padding_side = "left"
    eos_token_id = actor_tokenizer.eos_token_id
    pad_token_id = actor_tokenizer.pad_token_id
    prompt_list = [
        "What is 1+1?",
        "In PowerShell, how do I know if virtualization is disabled in BIOS?",
        "Why do people like swimming in aquariums instead of swimming pools?",
        "You are a marketing expert. Write 30 scripts with marketing techniques for Instagram reels.",
        "You are a marketing expert. Write 30 scripts with marketing techniques for Instagram reels.",
        "You are a marketing expert. Write 30 scripts with marketing techniques for Instagram reels.",
        "Why are all mirrors rectangular?",
        "What can we find in infected plant roots, ozone or gold?",
    ]
    prompts_dataset = PromptDataset(
        prompt_list, actor_tokenizer, apply_chat_template=True
    )
    prompts_dataloader = DataLoader(
        prompts_dataset, batch_size=rollout_batch_size, shuffle=True
    )

    train()
